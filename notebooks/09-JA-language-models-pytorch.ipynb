{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language model from scratch -- RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "path = untar_data(URLs.HUMAN_NUMBERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/jupyter/.fastai/data/human_numbers')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('valid.txt'),Path('train.txt')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = L()\n",
    "with open(path/'train.txt') as f: lines += L(*f.readlines())\n",
    "with open(path/'valid.txt') as f: lines += L(*f.readlines())\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#5) ['nine thousand nine hundred ninety five \\n','nine thousand nine hundred ninety six \\n','nine thousand nine hundred ninety seven \\n','nine thousand nine hundred ninety eight \\n','nine thousand nine hundred ninety nine \\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the numbers 1-9999 written out in english."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' . '.join([l.strip() for l in lines])\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) ['one','.','two','.','three','.','four','.','five','.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = L(text.split(' '))\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) ['twenty','one','.','twenty','two','.','twenty','three','.','twenty']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = L(*tokens).unique()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#63095) ['one','.','two','.','three','.','four','.','five','.'...],\n",
       " (#63095) [0,1,2,1,3,1,4,1,5,1...])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "nums = L(word2idx[i] for i in tokens)\n",
    "tokens, nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and take 3 tokens, and predict the fourth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#21031) [((#3) ['one','.','two'], '.'),((#3) ['.','three','.'], 'four'),((#3) ['four','.','five'], '.'),((#3) ['.','six','.'], 'seven'),((#3) ['seven','.','eight'], '.'),((#3) ['.','nine','.'], 'ten'),((#3) ['ten','.','eleven'], '.'),((#3) ['.','twelve','.'], 'thirteen'),((#3) ['thirteen','.','fourteen'], '.'),((#3) ['.','fifteen','.'], 'sixteen')...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L((tokens[i:i+3], tokens[i+3]) for i in range(0, len(tokens)-4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0, len(nums)-4, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `seqs` is our complete dataset, with the independent and dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`seqs` are legitimate `Datasets` because they have a length and we can index into them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=62, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = first(dls.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([62, 3]), torch.Size([62]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel1(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden) # input -> hidden\n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden) # hidden -> hidden\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz) # hidden -> output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.i_h(x[:,0])\n",
    "        h = F.relu(self.h_h(h))\n",
    "        h = h + self.i_h(x[:,1])\n",
    "        h = F.relu(self.h_h(h))\n",
    "        h = h + self.i_h(x[:,2])\n",
    "        h = F.relu(self.h_h(h))\n",
    "        return self.h_o(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First create an embedding for each word in the vocab with `n_hidden` latent variables \n",
    "\n",
    "Then, for each word in the mini-batch\n",
    "\n",
    "1. Pass the 1st embedding through a hidden layer \n",
    "1. Pass the embedding through a hidden layer\n",
    "1. Add the activation to the embedding vector of the second word\n",
    "1. Pass that through a hidden layer\n",
    "1. Add the activation to the embedding vector of the third word\n",
    "1. Pass that through a hidden layer\n",
    "1. Pass through a final linear layer to reshape the output to `vocab_sz`\n",
    "\n",
    "The final layer gives us a probability distribution over all words in the vocabulary.\n",
    "\n",
    "Diagramatically, we can represent the neural net like this:\n",
    "\n",
    "- input is rectangle\n",
    "- arrow is a computation\n",
    "- circle is computed activations\n",
    "- output is triangle\n",
    "- Two arrows impinging on a circle is an addition (or concatenation, either is fine -- though it'll change the shape of the hidden layers if you concatenate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./figures/rnn.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a sublety in the choice of architecture here: we use the **same neural net** `self.h_h` to go from hidden layer to hidden layer. This is because we expect there to be a single rule to transition from word-to-word in the language of human numbers. \n",
    "\n",
    "In my own wording, there needs to be a _spatial invariance_ to the model architecture. There should be no special significance to the position of the first, second, or third activations because there is no special significance to the first, second, or third position of the input data. What matters is how you transition between adjacent words/tokens. That's all.\n",
    "\n",
    "Question: Seems quite limiting in terms of the richness of the model. How do you improve on this...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.759525</td>\n",
       "      <td>1.980088</td>\n",
       "      <td>0.474210</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.391928</td>\n",
       "      <td>1.788412</td>\n",
       "      <td>0.474447</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.402349</td>\n",
       "      <td>1.661029</td>\n",
       "      <td>0.490611</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.365214</td>\n",
       "      <td>1.630936</td>\n",
       "      <td>0.478013</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(4, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this any good? A baseline would be to always predict the most common token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thousand', 7104),\n",
       " ('.', 7103),\n",
       " ('hundred', 6405),\n",
       " ('nine', 2440),\n",
       " ('eight', 2344)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter(tokens[cut:])\n",
    "mc = c.most_common(5)\n",
    "mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15353028894988222"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc[0][1]/len(tokens[cut:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you only get 15% accuracy when you always predict the token \"thousand\". We're doing substantially better than that so it is learning something of the structure of the language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refactor `LMModel1` to use a loop rather than writing out each line. This will have the benefit of allowing us to have token sequences of variable length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel2(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden) # input -> hidden\n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden) # hidden -> hidden\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz) # hidden -> output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = 0.  \n",
    "        for i in range(3):\n",
    "            h += self.i_h(x[:,i])\n",
    "            h = F.relu(self.h_h(h))                    \n",
    "        return self.h_o(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a bona-fide **recurrent neural network**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.850702</td>\n",
       "      <td>1.989384</td>\n",
       "      <td>0.466128</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.421135</td>\n",
       "      <td>1.794140</td>\n",
       "      <td>0.468267</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.423546</td>\n",
       "      <td>1.665611</td>\n",
       "      <td>0.493463</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.381168</td>\n",
       "      <td>1.674273</td>\n",
       "      <td>0.444260</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, \n",
    "                metrics=accuracy)\n",
    "learn.fit_one_cycle(4, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which can be depicted graphically as:\n",
    "\n",
    "<img src=\"./figures/rnn-ravel.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintaining the state of an RNN\n",
    "\n",
    "In `LMModel2`, we initialize the hidden state as `h = 0.`, which means that everything learnt up to that point is forgotten. But we're learning the full string of numbers from 0-9999, so it doesn't make much sense to throw away the state every time we get a new token sequence because there exists logical continuity between sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve this simply by having the hidden state as an attribute of the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel3(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h = 0.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(3):\n",
    "            self.h += self.i_h(x[:,i])\n",
    "            self.h = F.relu(self.h_h(self.h))\n",
    "        out = self.h_o(self.h)  # we only want to retain the hidden state\n",
    "        self.h = self.h.detach() # throw away gradient history     \n",
    "        return out  # we only want to return the output\n",
    "    \n",
    "    def reset(self): self.h = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need `self.h.detach()` because, without it, we'll be calculating the gradient of the loss with respect to `self.h` through _all previous mini-batches_. Whilst this is conceptually ok, computationally it's a bad idea -- it'll get very slow, take up a huge amount of memory, and potentially introduce a lot of instability due to the exploding gradients problem (or have no effect for all the work we've done, due to the vanishing gradients problem).\n",
    "\n",
    "We use `self.h.detach()` to throw away gradient history. \n",
    "\n",
    "- _Backpropagation through time (BPTT)_: Retaining the state between batches, but only having **a single hidden layer** per time step. \n",
    "- _Truncated BPTT_: Detaching the history of the computation of steps in the hidden state every few time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make sure that the samples are seen in the correct order so that the text flows properly through the model. `LMDataLoader` did this for us in the previous notebook but we'll do it ourselves here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328, 64, 21031)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = len(seqs)//bs\n",
    "m, bs, len(seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first batch should be composed of samples\n",
    "\n",
    "```\n",
    "(0, m, 2*m, ..., (bs-1)*m)\n",
    "```\n",
    "\n",
    "(where this vector is of length `bs`). The second batch should be composed of\n",
    "\n",
    "```\n",
    "(1, m+1, 2*m+1, ..., (bs-1)*m+1)\n",
    "```\n",
    "\n",
    "and so on `m` times. In this way, at each epoch, the model will see contiguous chunks of text of size `3*m` (since each token sequence consists of 3 tokens).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Note to self**_: Doesn't seem totally clear to me. What exactly is being done in parallel? At each mini-batch, 64 gradients are computed for the same model. These 64 gradients have nothing to do with each other. At this point, are the model parameters updated sequentially 64 times? Or is this stored in a computational tree, and only at the end of the epoch are the gradients for the model actually updated? That seems to be the implication of what they're saying, otherwise all this reshaping and batch assignment doesn't make any sense..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function does the reindexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_chunks(ds, bs):\n",
    "    m = len(ds) // bs\n",
    "    new_ds = L()\n",
    "    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n",
    "    return new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then drop the last batch which doesn't have the right shape, and do not shuffle the text to ensure text is read in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(\n",
    "    group_chunks(seqs[:cut], bs),\n",
    "    group_chunks(seqs[cut:], bs),\n",
    "    bs=bs,\n",
    "    drop_last=True,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start of each epoch we need to call `LMModel3.reset()`, because at the start of each epoch we are going back to the start of the natural numbers. So that means the activations `self.h` need to be reset.\n",
    "\n",
    "We also need to reset the activations when we perform validation.\n",
    "\n",
    "We can achieve this vai a `Callback` which say, during the training loop, train some particular code. We'll use `ModelResetter` which calls `model.reset` at\n",
    "\n",
    "- The beginning of training\n",
    "- The beginning of validation\n",
    "- After fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.721246</td>\n",
       "      <td>1.878376</td>\n",
       "      <td>0.395673</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.302078</td>\n",
       "      <td>1.694434</td>\n",
       "      <td>0.494952</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.119961</td>\n",
       "      <td>1.510426</td>\n",
       "      <td>0.519471</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.040456</td>\n",
       "      <td>1.578070</td>\n",
       "      <td>0.528846</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.992497</td>\n",
       "      <td>1.591788</td>\n",
       "      <td>0.549038</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.944943</td>\n",
       "      <td>1.618226</td>\n",
       "      <td>0.556250</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.884938</td>\n",
       "      <td>1.551036</td>\n",
       "      <td>0.582452</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.843339</td>\n",
       "      <td>1.621597</td>\n",
       "      <td>0.584135</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.812761</td>\n",
       "      <td>1.617809</td>\n",
       "      <td>0.601202</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.797448</td>\n",
       "      <td>1.622385</td>\n",
       "      <td>0.602644</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,\n",
    "                metrics=accuracy, cbs=ModelResetter\n",
    "               )\n",
    "learn.fit_one_cycle(10, 3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **stateful RNN**, which is much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating more signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problem with our current approach is that we only predict one output word for each 3 input words, which is quite wasteful. The amount of signal we feed back to update weights is not as large as it could be. It would be better to predict the next word after every single word, rather than every three words.\n",
    "\n",
    "<img src=\"./figures/rnn-signal.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, after every single state is created, do a prediction. Then our prediction will be the entire set of numbers, offset by 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, need to change our data such that each of the next 3 words are present for each of the 3 inputs.\n",
    "\n",
    "Let's change the sequence length (`sl`) from 3 to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#12618) [((#5) ['one','.','two','.','three'], (#5) ['.','two','.','three','.']),((#5) ['.','four','.','five','.'], (#5) ['four','.','five','.','six']),((#5) ['six','.','seven','.','eight'], (#5) ['.','seven','.','eight','.']),((#5) ['.','nine','.','ten','.'], (#5) ['nine','.','ten','.','eleven']),((#5) ['eleven','.','twelve','.','thirteen'], (#5) ['.','twelve','.','thirteen','.']),((#5) ['.','fourteen','.','fifteen','.'], (#5) ['fourteen','.','fifteen','.','sixteen']),((#5) ['sixteen','.','seventeen','.','eighteen'], (#5) ['.','seventeen','.','eighteen','.']),((#5) ['.','nineteen','.','twenty','.'], (#5) ['nineteen','.','twenty','.','twenty']),((#5) ['twenty','one','.','twenty','two'], (#5) ['one','.','twenty','two','.']),((#5) ['.','twenty','three','.','twenty'], (#5) ['twenty','three','.','twenty','four'])...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L((tokens[i:i+sl], tokens[i+1:i+sl+1])\n",
    "         for i in range(0, len(nums)-sl-1, sl)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = 16\n",
    "seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n",
    "         for i in range(0, len(nums)-sl-1, sl)\n",
    "        )\n",
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n",
    "                             group_chunks(seqs[cut:], bs),\n",
    "                             bs=bs,\n",
    "                             drop_last=True,\n",
    "                             shuffle=False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(#16) ['one','.','two','.','three','.','four','.','five','.'...],\n",
       " (#16) ['.','two','.','three','.','four','.','five','.','six'...]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[L(vocab[o] for o in s) for s in seqs[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to modify the model so that it outputs a prediction after every token, rathen than just at the end of a three-token sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel4(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h = 0.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for i in range(sl):\n",
    "            self.h = self.h + self.i_h(x[:,i])  # weird: += seems to confuse pytorch\n",
    "            self.h = F.relu(self.h_h(self.h))\n",
    "            outs.append(self.h_o(self.h)) # output for every token\n",
    "        self.h = self.h.detach() \n",
    "        return torch.stack(outs, dim=1) \n",
    "    \n",
    "    def reset(self): self.h = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model returns outputs of shape `bs x sl x vocab_sz` (since we stacked on dimension 1). \n",
    "\n",
    "Our targets are of shape `bs x sl`. So we need to flatten those before using them in `F.cross_entropy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(inp, targ):\n",
    "    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Not sure about the `.view` details here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.227117</td>\n",
       "      <td>3.160564</td>\n",
       "      <td>0.157959</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.776564</td>\n",
       "      <td>2.437939</td>\n",
       "      <td>0.388021</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.064298</td>\n",
       "      <td>1.950222</td>\n",
       "      <td>0.468913</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.675839</td>\n",
       "      <td>1.794347</td>\n",
       "      <td>0.474609</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.457691</td>\n",
       "      <td>1.874112</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.322245</td>\n",
       "      <td>1.743223</td>\n",
       "      <td>0.503255</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.203618</td>\n",
       "      <td>1.860203</td>\n",
       "      <td>0.497152</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.125153</td>\n",
       "      <td>1.816316</td>\n",
       "      <td>0.531982</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.053785</td>\n",
       "      <td>1.796995</td>\n",
       "      <td>0.528971</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.973118</td>\n",
       "      <td>2.010507</td>\n",
       "      <td>0.557292</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.916483</td>\n",
       "      <td>2.050870</td>\n",
       "      <td>0.550130</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.867749</td>\n",
       "      <td>1.876548</td>\n",
       "      <td>0.540120</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.821089</td>\n",
       "      <td>1.925179</td>\n",
       "      <td>0.554525</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.787424</td>\n",
       "      <td>2.107865</td>\n",
       "      <td>0.536540</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.746725</td>\n",
       "      <td>2.019338</td>\n",
       "      <td>0.583984</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.728124</td>\n",
       "      <td>2.118186</td>\n",
       "      <td>0.583008</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.675803</td>\n",
       "      <td>2.011272</td>\n",
       "      <td>0.577148</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.654459</td>\n",
       "      <td>1.952243</td>\n",
       "      <td>0.569417</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.618832</td>\n",
       "      <td>1.932300</td>\n",
       "      <td>0.535889</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.599287</td>\n",
       "      <td>2.035589</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.572809</td>\n",
       "      <td>1.942736</td>\n",
       "      <td>0.556152</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.563766</td>\n",
       "      <td>2.016213</td>\n",
       "      <td>0.549805</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.551982</td>\n",
       "      <td>1.918224</td>\n",
       "      <td>0.566732</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.521640</td>\n",
       "      <td>1.971379</td>\n",
       "      <td>0.575684</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.496675</td>\n",
       "      <td>2.041870</td>\n",
       "      <td>0.568848</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.484116</td>\n",
       "      <td>2.082114</td>\n",
       "      <td>0.581462</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.472121</td>\n",
       "      <td>2.096366</td>\n",
       "      <td>0.577799</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.464262</td>\n",
       "      <td>2.119890</td>\n",
       "      <td>0.579997</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.457103</td>\n",
       "      <td>2.070450</td>\n",
       "      <td>0.577474</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.452922</td>\n",
       "      <td>2.077253</td>\n",
       "      <td>0.579915</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(30, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.452704</td>\n",
       "      <td>2.109696</td>\n",
       "      <td>0.576335</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.463176</td>\n",
       "      <td>2.080742</td>\n",
       "      <td>0.576416</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.498154</td>\n",
       "      <td>2.062097</td>\n",
       "      <td>0.548096</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.511734</td>\n",
       "      <td>1.962567</td>\n",
       "      <td>0.610840</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.521158</td>\n",
       "      <td>2.204545</td>\n",
       "      <td>0.616618</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.529827</td>\n",
       "      <td>2.088227</td>\n",
       "      <td>0.602620</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.510220</td>\n",
       "      <td>1.900282</td>\n",
       "      <td>0.551188</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.489046</td>\n",
       "      <td>2.067856</td>\n",
       "      <td>0.589925</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.491480</td>\n",
       "      <td>2.287981</td>\n",
       "      <td>0.540609</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.458506</td>\n",
       "      <td>2.131739</td>\n",
       "      <td>0.588786</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.435969</td>\n",
       "      <td>2.183260</td>\n",
       "      <td>0.581787</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.416354</td>\n",
       "      <td>2.158975</td>\n",
       "      <td>0.598796</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.384192</td>\n",
       "      <td>2.330539</td>\n",
       "      <td>0.597005</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.371523</td>\n",
       "      <td>2.216208</td>\n",
       "      <td>0.620687</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.354557</td>\n",
       "      <td>2.307788</td>\n",
       "      <td>0.612467</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.340363</td>\n",
       "      <td>2.281833</td>\n",
       "      <td>0.620931</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.316364</td>\n",
       "      <td>2.112012</td>\n",
       "      <td>0.623698</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.309213</td>\n",
       "      <td>2.403499</td>\n",
       "      <td>0.604411</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.292337</td>\n",
       "      <td>2.215242</td>\n",
       "      <td>0.600098</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.274171</td>\n",
       "      <td>2.236285</td>\n",
       "      <td>0.636312</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.260933</td>\n",
       "      <td>2.261676</td>\n",
       "      <td>0.588379</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.239979</td>\n",
       "      <td>2.305764</td>\n",
       "      <td>0.620524</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.223125</td>\n",
       "      <td>2.341818</td>\n",
       "      <td>0.645182</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.220988</td>\n",
       "      <td>2.467985</td>\n",
       "      <td>0.609049</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.219781</td>\n",
       "      <td>2.278151</td>\n",
       "      <td>0.641439</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.207226</td>\n",
       "      <td>2.415808</td>\n",
       "      <td>0.617676</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.196915</td>\n",
       "      <td>2.381184</td>\n",
       "      <td>0.627197</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.190574</td>\n",
       "      <td>2.392117</td>\n",
       "      <td>0.631104</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.185749</td>\n",
       "      <td>2.434365</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.184022</td>\n",
       "      <td>2.425240</td>\n",
       "      <td>0.628174</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(30, 3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are quite variable because we have a very deep neural network here, resulting in unstable gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And yet we've only got one hidden layer, so the model can't do very sophisticated computations. The obvious thing to do is try having more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/rnn-multi.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dotted arrows are different weight matrices to the solid arrows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `nn.RNN` which is the same as `LMModel4` but lets us stack models conveniently ontop of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel5(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)        \n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h = torch.zeros(n_layers, bs, n_hidden)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res, h = self.rnn(self.i_h(x), self.h)        \n",
    "        self.h = h.detach() \n",
    "        return self.h_o(res)\n",
    "    \n",
    "    def reset(self): self.h.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.110875</td>\n",
       "      <td>2.665747</td>\n",
       "      <td>0.461670</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.174251</td>\n",
       "      <td>1.802367</td>\n",
       "      <td>0.471842</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.715677</td>\n",
       "      <td>1.908851</td>\n",
       "      <td>0.313314</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.498473</td>\n",
       "      <td>1.800743</td>\n",
       "      <td>0.454834</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.351316</td>\n",
       "      <td>1.773410</td>\n",
       "      <td>0.479085</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.242161</td>\n",
       "      <td>1.853326</td>\n",
       "      <td>0.498210</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.158545</td>\n",
       "      <td>2.038881</td>\n",
       "      <td>0.504150</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.081536</td>\n",
       "      <td>2.106154</td>\n",
       "      <td>0.503988</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.013214</td>\n",
       "      <td>2.225335</td>\n",
       "      <td>0.502034</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.955277</td>\n",
       "      <td>2.259951</td>\n",
       "      <td>0.506022</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.909459</td>\n",
       "      <td>2.286430</td>\n",
       "      <td>0.515706</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.876598</td>\n",
       "      <td>2.337988</td>\n",
       "      <td>0.513509</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.854115</td>\n",
       "      <td>2.324337</td>\n",
       "      <td>0.519206</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.840245</td>\n",
       "      <td>2.349777</td>\n",
       "      <td>0.516846</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.833168</td>\n",
       "      <td>2.349822</td>\n",
       "      <td>0.516764</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, LMModel5(len(vocab), 64, 2),\n",
    "                loss_func=CrossEntropyLossFlat(),\n",
    "               metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance isn't so good. Deep model means exploding/disappearing gradients. We can get around this with LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs\n",
    "\n",
    "Consists of cells containing mini neural networks, deciding how much of the previous state to keep/forget, how much of the input to keep, and how much of the \"cell state\" to keep/forget, which is a state which can zip straight through the network with few perturbations, so that long-term patterns can be learnt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won't build this from scratch for now, though the fastai book has this. We can just replace `nn.RNN` with `nn.LSTM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel6(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)        \n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res, h = self.rnn(self.i_h(x), self.h)        \n",
    "        self.h = [h_.detach()  for h_ in h]\n",
    "        return self.h_o(res)\n",
    "    \n",
    "    def reset(self): \n",
    "        for h in self.h: h.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.011936</td>\n",
       "      <td>2.767241</td>\n",
       "      <td>0.265544</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.232252</td>\n",
       "      <td>1.753886</td>\n",
       "      <td>0.455322</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.641269</td>\n",
       "      <td>2.023682</td>\n",
       "      <td>0.465902</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.387783</td>\n",
       "      <td>1.865891</td>\n",
       "      <td>0.506592</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.204617</td>\n",
       "      <td>2.271569</td>\n",
       "      <td>0.513753</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.997613</td>\n",
       "      <td>1.915654</td>\n",
       "      <td>0.601807</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.736215</td>\n",
       "      <td>1.439248</td>\n",
       "      <td>0.657389</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.505693</td>\n",
       "      <td>1.411785</td>\n",
       "      <td>0.689616</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.335138</td>\n",
       "      <td>1.361626</td>\n",
       "      <td>0.710286</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.212083</td>\n",
       "      <td>1.436214</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.129203</td>\n",
       "      <td>1.470226</td>\n",
       "      <td>0.741618</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.082437</td>\n",
       "      <td>1.467494</td>\n",
       "      <td>0.747721</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.058328</td>\n",
       "      <td>1.462406</td>\n",
       "      <td>0.748210</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.046365</td>\n",
       "      <td>1.435650</td>\n",
       "      <td>0.748291</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.041018</td>\n",
       "      <td>1.433158</td>\n",
       "      <td>0.748698</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, LMModel6(len(vocab), 64, 2), \n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. Overfitting a bit, let's use some regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizing an LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is particularly effective for RNNs. Dropout is simply zeroing the activations during training with probability `p`. To retain the scale of the output after zeroing activations with probability `p`, we rescale all activations by dividing them by `1-p`). At test time, all the activations are present, and they are left alone. You can also do it the other way around, like the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/dropout.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation regularization (AR) and temporal activation regularization (TAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explicitly penalize the activations from becoming too large by adding a regularization term to the _activations_ in the loss function (AR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "loss += alpha * activations.pow(2).mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also penalize the activations changing rapidly between time steps (TAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "loss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight-tied regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above model:\n",
    "\n",
    "- Input embedding = Conversion of token -> activation\n",
    "- Output hidden layer = Conversion from activation -> token\n",
    "\n",
    "Apparently, it is intuitive to think that these activations are the same (though in my mind it's more intuitive that they are encoding/decoding steps, so there's no reason they should be the same).\n",
    "\n",
    "Anyway, we can enforce this with\n",
    "\n",
    "```python\n",
    "self.h_o.weight = self.i_h.weight\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel7(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h_o.weight = self.i_h.weight # weight tying. The two objects will always be identical\n",
    "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raw,h = self.rnn(self.i_h(x), self.h)\n",
    "        out = self.drop(raw)\n",
    "        self.h = [h_.detach() for h_ in h]\n",
    "        return self.h_o(out),raw,out\n",
    "    \n",
    "    def reset(self): \n",
    "        for h in self.h: h.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n",
    "                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n",
    "                cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, more simply,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n",
    "                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add weight decay for good measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.597233</td>\n",
       "      <td>1.856889</td>\n",
       "      <td>0.481934</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.674434</td>\n",
       "      <td>1.300253</td>\n",
       "      <td>0.626872</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.997507</td>\n",
       "      <td>0.898619</td>\n",
       "      <td>0.744059</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.564185</td>\n",
       "      <td>0.707201</td>\n",
       "      <td>0.791504</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.355193</td>\n",
       "      <td>0.548807</td>\n",
       "      <td>0.834880</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.250618</td>\n",
       "      <td>0.543925</td>\n",
       "      <td>0.840251</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.198203</td>\n",
       "      <td>0.488984</td>\n",
       "      <td>0.856934</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.164151</td>\n",
       "      <td>0.432972</td>\n",
       "      <td>0.875407</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.143137</td>\n",
       "      <td>0.387605</td>\n",
       "      <td>0.888997</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.130521</td>\n",
       "      <td>0.427740</td>\n",
       "      <td>0.878011</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.120257</td>\n",
       "      <td>0.439557</td>\n",
       "      <td>0.875244</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.112262</td>\n",
       "      <td>0.409603</td>\n",
       "      <td>0.881266</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.106097</td>\n",
       "      <td>0.419092</td>\n",
       "      <td>0.877441</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.102155</td>\n",
       "      <td>0.440190</td>\n",
       "      <td>0.873372</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.099380</td>\n",
       "      <td>0.440970</td>\n",
       "      <td>0.872966</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(15, 1e-2, wd=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now all of the pieces for a AWD-LSTM."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
